{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello, Lab CA: Correspondence Analysis (CA) using prince on the French_elections dataset \n",
    "Lab Duration: 2-3 Hours \n",
    "Prerequisites: Basic understanding of Python, Pandas, Matplotlib, and Correspondence Analysis (CA) \n",
    " \n",
    "Lab Objectives \n",
    "By the end of this lab, students will be able to: \n",
    "1. Perform CA using the prince library. \n",
    "2. Show and interpret the eigenvalues, explained variance and cumulative explained variance. \n",
    "3. Get the new coordinates for rows (Row scores) and columns (Column scores) and show the data \n",
    "in the new Factor map. \n",
    "4. Get and interpret the column/row contributions to the total explained variance of each \n",
    "dimension.   \n",
    "5. Know the quality of representation of each point in the factor map. \n",
    " \n",
    "Lab Outline \n",
    "Part 1: Introduction to CA and the French Elections Voting Data \n",
    " Overview of CA. \n",
    " Description of the dataset (which comes with the prince library).  \n",
    "Part 2: Setting up the Environment and Loading Data  \n",
    " Installing required libraries (prince : version 0.13.1) \n",
    " Loading the French Elections Voting Data dataset using prince. \n",
    "Part 3: Performing CA using prince  \n",
    " Initializing the prince CA model. \n",
    " Fitting the model to the data. \n",
    " Getting the explained variance  \n",
    " Transforming the data and getting the new columns and rows coordinates  \n",
    "Part 4: Visualization & Interpretation \n",
    " Visualizing the data in the new dimensions (a 2-D space). \n",
    " Display rows and columns contributions.  \n",
    " \n",
    "Correspondence Analysis  Lab  (Ilyes Jenhani) \n",
    "2 \n",
    " \n",
    " Display the quality of representation of each data point on the factor maps (cosine similarities) \n",
    " Discussing the insights obtained from the plots. \n",
    "Part 5: Conclusion and Q&A  \n",
    " Summarize key points. \n",
    " \n",
    "Part 1: Introduction to CA and the French Elections Voting Data \n",
    "CA Overview \n",
    "Correspondence Analysis (CA)  \n",
    " Correspondence analysis is a useful data visualization technique for finding out and displaying \n",
    "the relationship between categories. It uses a graph that plots data, visually showing the \n",
    "outcome of two or more data points. \n",
    " CA uses a contingency tablea table of frequenciesthat shows how the categories of the \n",
    "variables are distributed. The data in the table undergoes a series of transformations in relation \n",
    "to the data around it to produce relational data. The resulting data is then plotted to show those \n",
    "relationships visually. \n",
    "The French Elections Voting Dataset \n",
    " This dataset counts the number of voters per region for each candidate in the 2022 French \n",
    "presidential elections. It can be used directly for correspondence analysis.  \n",
    " The dataset's name in this context is usually French Elections Voting Data, and it consists of a \n",
    "contingency table where the rows represent the regions or departments, and the columns \n",
    "represent the candidates. Each cell in the table holds the number of votes for a particular \n",
    "candidate in a specific region. \n",
    " \n",
    "Part 2: Setting up the Environment and Loading Data  \n",
    "Step 1: Install Required Libraries \n",
    " Install the required Python packages: \n",
    "pip install prince pandas seaborn matplotlib scikit-learn \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "Correspondence Analysis  Lab  (Ilyes Jenhani) \n",
    "3 \n",
    " \n",
    "Step 2: Load the Dataset and display its first 5 rows \n",
    " \n",
    "Question: what do you think about the format of dataset ? \n",
    "This dataset is already available as a contingency matrix. Its more common to have at ones disposal a \n",
    "flat dataset. If this is the case, a contigency matrix can be obtained using the pivot_table function in \n",
    "pandas. \n",
    " \n",
    "Part 3: Performing CA using prince \n",
    "Step 1: Initialize the CA Model \n",
    " Initialize CA with 2 components: \n",
    " \n",
    "Q. Explain the main parameters ?  \n",
    "Step 2: Fit the CA Model \n",
    " Fit the CA model to the dataset: \n",
    " \n",
    " Print eigenvalues, explained variance and total inertia of ca \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "Correspondence Analysis  Lab  (Ilyes Jenhani) \n",
    "4 \n",
    " \n",
    "Q. How is the percentage of explained variance calculated for each component? \n",
    " \n",
    "Step 3: Transforming the data and getting the new columns and rows coordinates  \n",
    "Get the rows (regions) and columns (candidates) coordinates.  \n",
    "Note that there is no ca.transform() as ca.row_coordinates() and ca.column_coordinates() already do \n",
    "the transformation.  \n",
    " \n",
    " \n",
    " \n",
    "Part 4: Visualization & Interpretation \n",
    "We can use the plot() function of prince library or use matplotlib (as we did in the PCA lab):  \n",
    " 1st visualization: Biplot showing the row and column factor maps: \n",
    " \n",
    " \n",
    "Q1. Explain the different parameters?  \n",
    "Q2. What do these factor maps show?  \n",
    " 2nd  visualization: How can we change the above parameters to get the following column factor \n",
    "map ? \n",
    " \n",
    " \n",
    "Correspondence Analysis  Lab  (Ilyes Jenhani) \n",
    "5 \n",
    " \n",
    " \n",
    " Displaying the contributions of columns to the 1st dimension in a barplot:  \n",
    " \n",
    " \n",
    " Displaying the contributions of rows to the 1st  dimension in a barplot.  \n",
    "Change the above code.  \n",
    " \n",
    " Getting the quality representation of each point in the 1st dimension:  \n",
    "- Cosine similarities of Columns (Candidates):  \n",
    " \n",
    " \n",
    "Correspondence Analysis  Lab  (Ilyes Jenhani) \n",
    "6 \n",
    " \n",
    " \n",
    " \n",
    "Part 5: Conclusions & QA \n",
    " Discuss how much variance is explained by the first two dimensions and the importance of each \n",
    "dimension. \n",
    " Discuss how columns (candidates) and rows (regions) contribute to the factors and what the plot \n",
    "tells us about the relationship about the two variables. \n",
    " \n",
    " \n",
    "Useful summary:  \n",
    "The final output of Correspondence Analysis (CA) consists of several key components that help interpret \n",
    "the relationships between the rows and columns of a contingency table (i.e., categorical variables). \n",
    "These outputs typically include: \n",
    " \n",
    "1. Row Coordinates (Row Scores) \n",
    "These are the coordinates of the rows (e.g., regions, departments) in the reduced factor space. They \n",
    "show how each row is represented on the principal dimensions (factors) derived from the analysis. \n",
    "These coordinates can be plotted to visualize how different rows are positioned relative to each other, \n",
    "indicating similarities or differences between the categories. \n",
    "2. Column Coordinates (Column Scores) \n",
    " \n",
    "Correspondence Analysis  Lab  (Ilyes Jenhani) \n",
    "7 \n",
    " \n",
    "These are the coordinates of the columns (e.g., candidates, categories) in the reduced factor space. \n",
    "Similar to row coordinates, they represent how each column relates to the principal dimensions. \n",
    "These scores can also be visualized on a scatter plot alongside the row coordinates, showing \n",
    "relationships between rows and columns. \n",
    "3. Eigenvalues and Explained Inertia \n",
    "Eigenvalues represent the amount of inertia (variance) captured by each dimension. Higher eigenvalues \n",
    "indicate dimensions that explain more of the association in the data. \n",
    "Explained inertia is the proportion of the total inertia (variance) explained by each dimension. It helps \n",
    "assess how much information is captured by the first few dimensions and determines the optimal \n",
    "number of dimensions to keep. \n",
    "4. Row Contributions \n",
    "The contribution of each row to the total inertia of a given dimension. This helps identify which rows \n",
    "(categories) are most responsible for the formation of each dimension. \n",
    "5. Column Contributions \n",
    "The contribution of each column to the total inertia of a dimension. This shows which columns \n",
    "(categories) have the greatest influence on a specific dimension. \n",
    "6. Row and Column Masses \n",
    "Row masses represent the relative importance (weight) of each row in the contingency table. \n",
    "Column masses represent the relative importance of each column. These masses are used in \n",
    "normalization during CA to account for uneven distributions. \n",
    "7. Factor Maps (Biplots) \n",
    "These are visual representations of the row and column coordinates on the first two or more \n",
    "dimensions. The row factor map shows the distribution of rows in the reduced space, while the column \n",
    "factor map shows the positioning of columns. \n",
    "A biplot can display both row and column coordinates on the same graph, showing the associations \n",
    "between rows and columns in the reduced dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install prince pandas seaborn matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prince import CA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setting up the Environment and Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset and display its first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('https://github.com/ilyesjenhani/CA-Lab/raw/main/french_elections.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: what do you think about the format of dataset ?**\n",
    "\n",
    "This dataset is already available as a contingency matrix. It's more common to have at one's disposal a flat dataset. If this is the case, a contingency matrix can be obtained using the `pivot_table` function in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Performing CA using prince"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initialize the CA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = CA(n_components=2, n_iter=3, copy=True, check_input=True, engine='auto', random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. Explain the main parameters ?**\n",
    "\n",
    "* `n_components`: The number of dimensions to keep after the CA transformation (default is 2).\n",
    "* `n_iter`: The number of iterations for the power iteration algorithm (default is 3).\n",
    "* `copy`: Whether to copy the input data or modify it in place (default is True).\n",
    "* `check_input`: Whether to check the input data for validity (default is True).\n",
    "* `engine`: The engine to use for the computation ('auto', 'sklearn', or 'eigen' - default is 'auto').\n",
    "* `random_state`: The seed for the random number generator (default is None)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fit the CA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca = ca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print eigenvalues, explained variance and total inertia of ca\n",
    "print(ca.eigenvalues_)\n",
    "print(ca.explained_inertia_)\n",
    "print(ca.total_inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. How is the percentage of explained variance calculated for each component?**\n",
    "\n",
    "The percentage of explained variance for each component is calculated by dividing the eigenvalue of that component by the total inertia and multiplying by 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Transforming the data and getting the new columns and rows coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rows (regions) and columns (candidates) coordinates.\n",
    "row_coordinates = ca.row_coordinates(X)\n",
    "col_coordinates = ca.column_coordinates(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that there is no `ca.transform()` as `ca.row_coordinates()` and `ca.column_coordinates()` already do the transformation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualization & Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `plot()` function of prince library or use matplotlib (as we did in the PCA lab):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st visualization: Biplot showing the row and column factor maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ca.plot_coordinates(\n",
    "     X=X,\n",
    "     ax=None,\n",
    "     figsize=(6, 6),\n",
    "     x_component=0,\n",
    "     y_component=1,\n",
    "     show_row_labels=True,\n",
    "     show_col_labels=True\n",
    " )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. Explain the different parameters?**\n",
    "\n",
    "* `X`: The input data.\n",
    "* `ax`: The matplotlib axes to plot on (default is None).\n",
    "* `figsize`: The size of the figure (default is (6, 6)).\n",
    "* `x_component`: The component to plot on the x-axis (default is 0).\n",
    "* `y_component`: The component to plot on the y-axis (default is 1).\n",
    "* `show_row_labels`: Whether to show the row labels (default is True).\n",
    "* `show_col_labels`: Whether to show the column labels (default is True)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. What do these factor maps show?**\n",
    "\n",
    "These factor maps show the relationships between the rows (regions) and columns (candidates) in the reduced dimensional space. The closer two points are on the map, the more similar they are in terms of their voting patterns. The further apart two points are, the more dissimilar they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd visualization: How can we change the above parameters to get the following column factor map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ca.plot_coordinates(\n",
    "     X=X,\n",
    "     ax=None,\n",
    "     figsize=(6, 6),\n",
    "     x_component=0,\n",
    "     y_component=1,\n",
    "     show_row_labels=False,\n",
    "     show_col_labels=True\n",
    " )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the column factor map only, we set `show_row_labels=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the contributions of columns to the 1st dimension in a barplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.column_contributions_.sort_values(by=['Dim. 1'], ascending=False, inplace=True)\n",
    "sns.barplot(x=ca.column_contributions_['Dim. 1'], y=ca.column_contributions_.index)\n",
    "plt.title('Contributions of columns to Dim.1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the contributions of rows to the 1st dimension in a barplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca.row_contributions_.sort_values(by=['Dim. 1'], ascending=False, inplace=True)\n",
    "sns.barplot(x=ca.row_contributions_['Dim. 1'], y=ca.row_contributions_.index)\n",
    "plt.title('Contributions of rows to Dim.1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the quality representation of each point in the 1st dimension:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Cosine similarities of Columns (Candidates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos2_cols = ca.column_cosine_similarities_\n",
    "print(cos2_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Conclusions & QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss how much variance is explained by the first two dimensions and the importance of each dimension.**\n",
    "\n",
    "**Discuss how columns (candidates) and rows (regions) contribute to the factors and what the plot tells us about the relationship about the two variables.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful summary:**\n",
    "\n",
    "The final output of Correspondence Analysis (CA) consists of several key components that help interpret the relationships between the rows and columns of a contingency table (i.e., categorical variables). These outputs typically include:\n",
    "\n",
    "**1. Row Coordinates (Row Scores)**\n",
    "These are the coordinates of the rows (e.g., regions, departments) in the reduced factor space. They show how each row is represented on the principal dimensions (factors) derived from the analysis. These coordinates can be plotted to visualize how different rows are positioned relative to each other, indicating similarities or differences between the categories.\n",
    "**2. Column Coordinates (Column Scores)**\n",
    "These are the coordinates of the columns (e.g., candidates, categories) in the reduced factor space. Similar to row coordinates, they represent how each column relates to the principal dimensions. These scores can also be visualized on a scatter plot alongside the row coordinates, showing relationships between rows and columns.\n",
    "**3. Eigenvalues and Explained Inertia**\n",
    "Eigenvalues represent the amount of inertia (variance) captured by each dimension. Higher eigenvalues indicate dimensions that explain more of the association in the data. Explained inertia is the proportion of the total inertia (variance) explained by each dimension. It helps assess how much information is captured by the first few dimensions and determines the optimal number of dimensions to keep.\n",
    "**4. Row Contributions**\n",
    "The contribution of each row to the total inertia of a given dimension. This helps identify which rows (categories) are most responsible for the formation of each dimension.\n",
    "**5. Column Contributions**\n",
    "The contribution of each column to the total inertia of a dimension. This shows which columns (categories) have the greatest influence on a specific dimension.\n",
    "**6. Row and Column Masses**\n",
    "Row masses represent the relative importance (weight) of each row in the contingency table. Column masses represent the relative importance of each column. These masses are used in normalization during CA to account for uneven distributions.\n",
    "**7. Factor Maps (Biplots)**\n",
    "These are visual representations of the row and column coordinates on the first two or more dimensions. The row factor map shows the distribution of rows in the reduced space, while the column factor map shows the positioning of columns. A biplot can display both row and column coordinates on the same graph, showing the associations between rows and columns in the reduced dimensional space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
